{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4fa8d2-356e-43d7-a9ab-a066dfaf82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eba2a",
   "metadata": {},
   "source": [
    "# Training, Tuning and Deploying a PyTorch Text Classification Model on [Vertex AI](https://cloud.google.com/vertex-ai)\n",
    "## Fine-tuning pre-trained [BERT](https://huggingface.co/bert-base-cased) model for sentiment classification task "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb063228",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This example is inspired from Token-Classification [notebook](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb) and [run_glue.py](https://github.com/huggingface/transformers/blob/v2.5.0/examples/run_glue.py). \n",
    "We will be fine-tuning **`bert-base-cased`** (pre-trained) model for sentiment classification task.\n",
    "You can find the details about this model at [Hugging Face Hub](https://huggingface.co/bert-base-cased).\n",
    "\n",
    "For more notebooks with the state of the art PyTorch/Tensorflow/JAX, you can explore [Hugging FaceNotebooks](https://huggingface.co/transformers/notebooks.html).\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will be using [IMDB movie review dataset](https://huggingface.co/datasets/imdb) from [Hugging Face Datasets](https://huggingface.co/datasets).\n",
    "\n",
    "### Objective\n",
    "\n",
    "How to **Build, Train, Tune and Deploy PyTorch models on [Vertex AI](https://cloud.google.com/vertex-ai)** and emphasize first class support for training and deploying PyTorch models on Vertex AI. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "This notebook covers following sections:\n",
    "\n",
    "- [Creating Notebooks instance](#Creating-Notebooks-instance-on-Google-Cloud)\n",
    "- [Training](#Training)\n",
    "    - [Run Training Locally in the Notebook](#Training-locally-in-the-notebook)\n",
    "    - [Run Training Job on Vertex AI](#Training-on-Vertex-AI)\n",
    "        - [Training with pre-built container](#Run-Custom-Job-on-Vertex-AI-Training-with-a-pre-built-container)\n",
    "        - [Training with custom container](#Run-Custom-Job-on-Vertex-AI-Training-with-custom-container)\n",
    "- [Tuning](#Hyperparameter-Tuning) \n",
    "    - [Run Hyperparameter Tuning job on Vertex AI](#Run-Hyperparameter-Tuning-Job-on-Vertex-AI)\n",
    "- [Deploying](#Deploying)\n",
    "    - [Deploying model on Vertex AI Predictions with custom container](#Deploying-model-on-Vertex AI-Predictions-with-custom-container)\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
    "\n",
    "* [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)\n",
    "* [Vertex AI Training](https://cloud.google.com/vertex-ai/docs/training/custom-training)\n",
    "* [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions)\n",
    "* [Cloud Storage](https://cloud.google.com/storage)\n",
    "* [Container Registry](https://cloud.google.com/container-registry)\n",
    "* [Cloud Build](https://cloud.google.com/build) *[Optional]*\n",
    "\n",
    "Learn about [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage Pricing](https://cloud.google.com/storage/pricing) and [Cloud Build Pricing](https://cloud.google.com/build/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30524f",
   "metadata": {},
   "source": [
    "## Creating Notebooks instance on Google Cloud\n",
    "\n",
    "This notebook assumes you are working with [PyTorch 1.9 DLVM](https://cloud.google.com/ai-platform/notebooks/docs/images) development environment with GPU runtime. You can create a Notebook instance using [Google Cloud Console](https://cloud.google.com/notebooks/docs/create-new) or [`gcloud` command](https://cloud.google.com/sdk/gcloud/reference/notebooks/instances/create).\n",
    "\n",
    "```\n",
    "gcloud notebooks instances create example-instance \\\n",
    "    --vm-image-project=deeplearning-platform-release \\\n",
    "    --vm-image-family=pytorch-1-9-cu110-notebooks \\\n",
    "    --machine-type=n1-standard-4 \\\n",
    "    --location=us-central1-a \\\n",
    "    --boot-disk-size=100 \\\n",
    "    --accelerator-core-count=1 \\\n",
    "    --accelerator-type=NVIDIA_TESLA_V100 \\\n",
    "    --install-gpu-driver \\\n",
    "    --network=default\n",
    "```\n",
    "***\n",
    "**NOTE:** You must have GPU quota before you can create instances with GPUs. Check the [quotas](https://console.cloud.google.com/iam-admin/quotas) page to ensure that you have enough GPUs available in your project. If GPUs are not listed on the quotas page or you require additional GPU quota, [request a quota increase](https://cloud.google.com/compute/quotas#requesting_additional_quota). Free Trial accounts do not receive GPU quota by default.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048be0ee",
   "metadata": {},
   "source": [
    "### The original notebook presents different instruction sets for running on Google Colab, Google Cloud Notebooks (Vertex Workbench), or locally\n",
    "\n",
    "#### Here we adjust the code for the Vertex Workbench case\n",
    "#### Copy the following code to a Terminal window to spin-up a Vertex Workbench User-managed Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1681e1",
   "metadata": {},
   "source": [
    "```\n",
    "gcloud notebooks instances create pytorch-1-9-t4-gpu-instance \\\n",
    "    --vm-image-project=deeplearning-platform-release \\\n",
    "    --vm-image-family=pytorch-1-9-cu110-notebooks \\\n",
    "    --machine-type=n1-standard-4 \\\n",
    "    --location=us-west4-a \\\n",
    "    --boot-disk-size=100 \\\n",
    "    --accelerator-core-count=1 \\\n",
    "    --accelerator-type=NVIDIA_TESLA_T4 \\\n",
    "    --install-gpu-driver \\\n",
    "    --network=default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774651cd",
   "metadata": {},
   "source": [
    "### Then, enter the JupyterLab environment in the Notebook instance and run the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eeed55-4983-4c5e-8eb3-8b33f79a170f",
   "metadata": {},
   "source": [
    "### Install additional packages\n",
    "\n",
    "Python dependencies required for this notebook are [Transformers](https://pypi.org/project/transformers/), [Datasets](https://pypi.org/project/datasets/) and [hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) will be installed in the Notebooks instance itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f60e99-6bdf-46d1-85a2-1a66257de0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be123a44-5b6f-407b-90e5-9d6c65f7dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c674d94f-d021-4bb4-86ea-0f643e527e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc09c55c-6a2f-4d62-8626-778ef2bda569",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90419656-737f-4ce8-a173-b8f967db2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5684477-4bac-4bbc-a820-7d911b068d20",
   "metadata": {},
   "source": [
    "We will be using [Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/start/client-libraries#python) to interact with Vertex AI services. The high-level `aiplatform` library is designed to simplify common data science workflows by using wrapper classes and opinionated defaults. \n",
    "\n",
    "#### Install Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b9db7c-1a2a-41e0-b3b4-d5450f4dcb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7821378-8c0c-44b2-8d0d-0725cc3c6bbc",
   "metadata": {},
   "source": [
    "### Restart the Kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0e9e26-8629-47a6-b579-992688d5df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5dbe99-28ae-4c45-bd4e-04d1cc6941c4",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Select a GPU runtime\n",
    "\n",
    "**Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select \"Runtime --> Change runtime type > GPU\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613996d1-8bce-4a09-8ed0-70ac780ecd15",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "1. Enable following APIs in your project required for running the tutorial\n",
    "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
    "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
    "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c89de-2cb0-4162-bec2-29e9d75a16e6",
   "metadata": {},
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud` or `google.auth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95232de-90bf-4697-8b41-11b4a6c26056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  spheric-rhythm-234515\n",
      "Please set your project id before proceeding to next step. Currently it's set as spheric-rhythm-234515\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"spheric-rhythm-234515\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID using google.auth\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import google.auth\n",
    "\n",
    "    _, PROJECT_ID = google.auth.default()\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "# validate PROJECT_ID\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"spheric-rhythm-234515\":\n",
    "    print(\n",
    "        f\"Please set your project id before proceeding to next step. Currently it's set as {PROJECT_ID}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80a9c9-b174-4dd0-9fb5-5f078418b2f0",
   "metadata": {},
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key** page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "2. Click **Create service account**.\n",
    "3. In the **Service account name** field, enter a name, and click **Create**.\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your local environment.\n",
    "6. Enter the path to your service account key as the `GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb9db2",
   "metadata": {},
   "source": [
    "### service account already created with the following command (from a previous laboratory)\n",
    "#### https://github.com/garcianava/vertex-ai-training/tree/main/02_vertex_ai_qwikstart\n",
    "\n",
    "### vertex-custom-training-sa@spheric-rhythm-234515.iam.gserviceaccount.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc70c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "985bc741-02db-4a86-8e6f-47907442cfc7",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package containing your training code to a Cloud Storage bucket. Vertex AI runs the code from this package. In this tutorial, Vertex AI also saves the trained model that results from your job in the same bucket. Using this model artifact, you can then create Vertex AI model and endpoint resources in order to serve online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ddddf61-74cc-4e0f-9cd9-914d59dddf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://pytorch-text-classification-bucket\"\n",
    "REGION = \"us-west4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1adf9f-5c75-4e01-a7db-4cdbad169184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID = spheric-rhythm-234515\n",
      "BUCKET_NAME = gs://pytorch-text-classification-bucket\n",
      "REGION = us-west4\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT_ID = {PROJECT_ID}\")\n",
    "print(f\"BUCKET_NAME = {BUCKET_NAME}\")\n",
    "print(f\"REGION = {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f42d98-f34a-44f6-9296-73e4e0021194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://pytorch-text-classification-bucket/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc6f1d5-7c05-4917-ac71-357998381ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadace89-caf2-446b-be63-c00b25f175ea",
   "metadata": {},
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e809bb5f-d0ca-4a62-a59d-25d25233b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03688d55-c822-4781-84bd-1da1512422f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8399c49-8214-408b-b744-d67f295807ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\n/opt/conda/lib/python3.10/site-packages/_XLAC.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m     get_reporting_integration_callbacks,\n\u001b[1;32m     41\u001b[0m     hp_params,\n\u001b[1;32m     42\u001b[0m     is_fairscale_available,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/integrations.py:71\u001b[0m\n\u001b[1;32m     69\u001b[0m             _has_neptune \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProgressCallback, TrainerCallback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PREFIX_CHECKPOINT_DIR, BestRun, IntervalStrategy  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer_callback.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntervalStrategy, has_length\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/training_args.py:71\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_model\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxm\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_neuroncore_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# torchrun support\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/xla/pull/3609\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_xla/__init__.py:186\u001b[0m\n\u001b[1;32m    185\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPU_LOAD_LIBRARY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_XLAC\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPU_LOAD_LIBRARY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: /opt/conda/lib/python3.10/site-packages/_XLAC.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassLabel, Sequence, load_dataset\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AutoModelForSequenceClassification, AutoTokenizer,\n\u001b[1;32m      8\u001b[0m                           EvalPrediction, Trainer, TrainingArguments,\n\u001b[1;32m      9\u001b[0m                           default_data_collator)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1089\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1101\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\n/opt/conda/lib/python3.10/site-packages/_XLAC.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import ClassLabel, Sequence, load_dataset\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          EvalPrediction, Trainer, TrainingArguments,\n",
    "                          default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b198c75-dc28-4a27-9d3e-45e2d65cc288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: CPU\n",
      "PyTorch version : 2.0.1+cu117\n",
      "Transformers version : 2.14.2\n",
      "Datasets version : 4.31.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook runtime: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Transformers version : {datasets.__version__}\")\n",
    "print(f\"Datasets version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "619627b5-f805-46f8-8cbc-51f220377716",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"finetuned-bert-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5daef6ee-442e-4767-8c79-c5dc1caca289",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fa32d-f2aa-4532-8a66-ae0b3dfb5499",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this section, we will train a PyTorch model by fine-tuning pre-trained model from [Hugging Face Transformers](https://github.com/huggingface/transformers). We will train the model locally first and then on [Vertex AI training service](https://cloud.google.com/vertex-ai/docs/training/custom-training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391431d4-a404-482d-b496-5f8d5b3af112",
   "metadata": {},
   "source": [
    "## Training locally in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976497d-ac03-4448-b0ae-3d99c59cdde2",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddad1a-00d9-4a8e-9d62-0c46e6c41b51",
   "metadata": {},
   "source": [
    "For this example we will use [IMDB movie review dataset](https://huggingface.co/datasets/imdb) from [Hugging Face Datasets](https://huggingface.co/datasets/) for sentiment classification task. We use the [Hugging Face Datasets](https://github.com/huggingface/datasets) library to download the data. This can be easily done with the function `load_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b09c38d-fe72-47ae-90dd-bc366ff2e0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dacaeb1f314d4abc3fe4127bc1667a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27220f6fb59c4200843765680728a204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8b171c80ea4ebbabe3672a56fe1080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098f3ddf5c934f32a4cf5371dc656107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b59832d862480eb7fe587dc2d774a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6fc146b8364bebb630501e2eaffe50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa922ce671d4049a2dc7c22f17acad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea93d5-fd69-40aa-b11a-8f52fa5156e9",
   "metadata": {},
   "source": [
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef8bc79d-7733-477b-98f8-c87647c2d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of rows in training dataset 25000 and size 207.25 MB\n",
      "Total # of rows in test dataset 25000 and size 207.25 MB\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Total # of rows in training dataset {} and size {:5.2f} MB\".format(\n",
    "        dataset[\"train\"].shape[0], dataset[\"train\"].size_in_bytes / (1024 * 1024)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Total # of rows in test dataset {} and size {:5.2f} MB\".format(\n",
    "        dataset[\"test\"].shape[0], dataset[\"test\"].size_in_bytes / (1024 * 1024)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569b653-58d9-47b0-9372-5f9ec3cc2440",
   "metadata": {},
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34984cda-c197-464b-a12f-64c0c0a8a1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090d4d7-a56d-4755-98ad-4dc236db1534",
   "metadata": {},
   "source": [
    "Using the `unique` method to extract label list. This will allow us to experiment with other datasets without hard-coding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac5f246b-559d-4425-be65-367baa5cf40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].unique(\"label\")\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7875ae1-3e86-4015-9782-af249baa932c",
   "metadata": {},
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset (automatically decoding the labels in passing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d97d7516-8ae3-4d5e-b0c6-eafba4b7f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=2):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(\n",
    "                lambda x: [typ.feature.names[i] for i in x]\n",
    "            )\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f61bfee5-d3b2-419c-8612-4b3566e8c992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First off, I hadn't seen \"The Blob\" since I was 7 or 8 and viewing it as an adult was an incredible experience. Pages could be written on its influence on horror films even today. And even more could be written on its social subtext with the 50s \"fear of teenagers\". But this simple little tale of interplanetary horror is still a damn fine scary movie if you let it be.&lt;br /&gt;&lt;br /&gt;Sure, it looks cheesy as all get out in our modern world. But \"The Blob\" packs in some genuinely frightening moments as a band of kids track the unstoppable creature when then adults don't believe them. In fact, there are even some pretty bleak moments in its candy-colored world. And Steve McQueen gives so much more than the story deserved on paper that we the viewers really get caught in the moment and believe in him.&lt;br /&gt;&lt;br /&gt;To sum up, if you can take off your postmodern irony filter, there's a lot more to love here than meets the eye.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thoroughly enjoyed this true to form take on the Dick Tracy persona. This is a well done product that used modern technology to craft a imagery filled comic era story. If you are a fan of or recently watched some of the old Dick Tracy b&amp;w movies then you're sure to get a kick out of this rendition. The pastel colors and larger than life characters rendered in a painstakingly authentic take on an era gone by is entertainment as it's meant to be. I personally find Madonna's musical element to be a major part of this film-the CD featuring her music from this movie is one I've listened to often over the years, it's just so well done and performed musically and tuned to that era. In my mind, Madonna's finest moment both on-screen but especially musically. This is sure to bring out the \"kid\" in you.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94f5de",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1183f09",
   "metadata": {},
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a pre-trained Hugging Face Transformers [`Tokenizer` class](https://huggingface.co/transformers/main_classes/tokenizer.html) which tokenizes the inputs (including converting the tokens to their corresponding IDs in the pre-trained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which ensures:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pre-training this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df06f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_seq_length = 128\n",
    "model_name_or_path = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1b330-f7a0-4e72-bb3e-0e3d2b5b3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "# 'use_fast' ensure that we use fast tokenizers (backed by Rust) from the 🤗 Tokenizers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce49aa",
   "metadata": {},
   "source": [
    "You can check type of models available with a fast tokenizer on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f5ad1",
   "metadata": {},
   "source": [
    "You can directly call this tokenizer on one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3dc52",
   "metadata": {},
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "**NOTE:** If, as is the case here, your inputs have already been split into words, you should pass the list of words to your tokenizer with the argument `is_split_into_words=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a43d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][4]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd9570-83f3-49e6-a7fa-ca717fc09b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\n",
    "    [\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"],\n",
    "    is_split_into_words=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2513a7",
   "metadata": {},
   "source": [
    "Note that transformers are often pre-trained with sub-word tokenizers, meaning that even if your inputs have been split into words already, each of those words could be split again by the tokenizer. Let's look at an example of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a950c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading repeated here to make this cell idempotent\n",
    "# Since we are over-writing datasets variable\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Mapping labels to ids\n",
    "# NOTE: We can extract this automatically but the `Unique` method of the datasets\n",
    "# is not reporting the label -1 which shows up in the pre-processing.\n",
    "# Hence the additional -1 term in the dictionary\n",
    "label_to_id = {1: 1, 0: 0, -1: 0}\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the input example texts\n",
    "    NOTE: The same preprocessing step(s) will be applied\n",
    "    at the time of inference as well.\n",
    "    \"\"\"\n",
    "    args = (examples[\"text\"],)\n",
    "    result = tokenizer(\n",
    "        *args, padding=\"max_length\", max_length=max_seq_length, truncation=True\n",
    "    )\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [label_to_id[example] for example in examples[\"label\"]]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# apply preprocessing function to input examples\n",
    "dataset = dataset.map(preprocess_function, batched=True, load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d507d0",
   "metadata": {},
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874cb91",
   "metadata": {},
   "source": [
    "Now that our data is ready, we can download the pre-trained model and fine-tune it.\n",
    "\n",
    "---\n",
    "\n",
    "***Fine Tuning*** *involves taking a model that has already been trained for a given task and then tweaking the model for another similar task. Specifically, the tweaking involves replicating all the layers in the pre-trained model including weights and parameters, except the output layer. Then adding a new output classifier layer that predicts labels for the current task. The final step is to train the output layer from scratch, while the parameters of all layers from the pre-trained model are frozen. This allows learning from the pre-trained representations and \"fine-tuning\" the higher-order feature representations more relevant for the concrete task, such as analyzing sentiments in this case.* \n",
    "\n",
    "*For the scenario in the notebook analyzing sentiments, the pre-trained BERT model already encodes a lot of information about the language as the model was trained on a large corpus of English data in a self-supervised fashion. Now we only need to slightly tune them using their outputs as features for the sentiment classification task. This means quicker development iteration on a much smaller dataset, instead of training a specific Natural Language Processing (NLP) model with a larger training dataset.*\n",
    "\n",
    "\n",
    "![BERT fine-tuning](./images/bert-model-fine-tuning.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b6e89",
   "metadata": {},
   "source": [
    "Since all our tasks are about token classification, we use the `AutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which we can get from the features, as seen before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path, num_labels=len(label_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437accf",
   "metadata": {},
   "source": [
    "**NOTE:** The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pre-train the model on a masked language modeling objective and replacing it with a new head for which we don't have pre-trained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33aa2d",
   "metadata": {},
   "source": [
    "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    output_dir=\"/tmp/cls\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d1dfb",
   "metadata": {},
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f614252",
   "metadata": {},
   "source": [
    "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a predictions and label_ids field) and has to return a dictionary string to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589f8d6",
   "metadata": {},
   "source": [
    "Now we create the `Trainer` object and we are almost ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb2b3e",
   "metadata": {},
   "source": [
    "You can add callbacks to the `trainer` object to customize the behavior of the training loop such as early stopping, reporting metrics at the end of evaluation phase or taking any decisions. In the hyperparameter tuning section of this notebook, we add a callback to `trainer` for automating hyperparameter tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63202442",
   "metadata": {},
   "source": [
    "We can now fine-tune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_local_path = \"./models\"\n",
    "!mkdir ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(saved_model_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cec3d8",
   "metadata": {},
   "source": [
    "The `evaluate` method allows you to evaluate again on the evaluation dataset or on another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaabc2f",
   "metadata": {},
   "source": [
    "To get the other metrics computed  such as precision, recall or F1 score for each category, we can apply the same function as before on the result of the `predict` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b321d3",
   "metadata": {},
   "source": [
    "### Run predictions locally with sample examples\n",
    "\n",
    "Using the trained model, we can predict the sentiment label for an input text after applying the preprocessing function that was used during the training. We will run the predictions locally in the notebook and later show how you can deploy the model to an endpoint using [TorchServe](https://pytorch.org/serve/) on Vertex AI Predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"bert-base-cased\"\n",
    "label_text = {0: \"Negative\", 1: \"Positive\"}\n",
    "saved_model_path = saved_model_local_path\n",
    "\n",
    "\n",
    "def predict(input_text, saved_model_path):\n",
    "    # initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "    # preprocess and encode input text\n",
    "    tokenizer_args = (input_text,)\n",
    "    predict_input = tokenizer(\n",
    "        *tokenizer_args,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # load trained model\n",
    "    loaded_model = AutoModelForSequenceClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "    # get predictions\n",
    "    output = loaded_model(predict_input[\"input_ids\"])\n",
    "\n",
    "    # return labels\n",
    "    label_id = torch.argmax(*output.to_tuple(), dim=1)\n",
    "\n",
    "    print(f\"Review text: {input_text}\")\n",
    "    print(f\"Sentiment : {label_text[label_id.item()]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd67c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example #1\n",
    "review_text = (\n",
    "    \"\"\"Jaw dropping visual affects and action! One of the best I have seen to date.\"\"\"\n",
    ")\n",
    "predict_input = predict(review_text, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example #2\n",
    "review_text = \"\"\"Take away the CGI and the A-list cast and you end up with film with less punch.\"\"\"\n",
    "predict_input = predict(review_text, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24477b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c5f05bd",
   "metadata": {},
   "source": [
    "## Training on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5525b3f",
   "metadata": {},
   "source": [
    "You can do local experimentation on your Notebooks instance. However, for larger datasets or models often a vertically scaled compute or horizontally distributed training is required. The most effective way to perform this task is to leverage [Vertex AI custom training service](https://cloud.google.com/vertex-ai/docs/training/custom-training) for following reasons:\n",
    "\n",
    "- **Automatically provision and de-provision resources**: Training job on Vertex AI will automatically provision computing resources, performs the training task and ensures deletion of compute resources once the training job is finished.\n",
    "- **Reusability and portability**: You can package training code with its parameters and dependencies into a container and create a portable component. This container can then be run with different scenarios such as hyperparameter tuning, different data sources and more.\n",
    "- **Training at scale**: You can run a [distributed training job](https://cloud.google.com/vertex-ai/docs/training/distributed-training) with AI allowing you to train models in a cluster across multiple nodes in parallel and resulting in faster training time. \n",
    "- **Logging and Monitoring**: The training service logs messages from the job to [Cloud Logging](https://cloud.google.com/logging/docs) and can be monitored while the job is running.\n",
    "\n",
    "In this part of the notebook, we show how to scale the training job with Vertex AI by packaging the code and create a training pipeline to orchestrate a training job. There are three steps to run a training job using [Vertex AI custom training service](https://cloud.google.com/vertex-ai/docs/training/custom-training):\n",
    "\n",
    "- **STEP 1**: Determine training code structure - Packaging as a Python source distribution or as a custom container image\n",
    "- **STEP 2**: Chose a custom training method - custom job, hyperparameter training job or training pipeline\n",
    "- **STEP 3**: Run the training job\n",
    "\n",
    "![custom-training-on-vertex-ai](./images/custom-training-on-vertex-ai.png)\n",
    "\n",
    "#### Custom training methods\n",
    "\n",
    "There are three types of Vertex AI resources you can create to train custom models on Vertex AI:\n",
    "\n",
    "- **[Custom jobs](https://cloud.google.com/vertex-ai/docs/training/create-custom-job):** With a custom job you configure the settings to run your training code on Vertex AI such as worker pool specs - machine types, accelerators, Python training spec or custom container spec. \n",
    "- **[Hyperparameter tuning jobs](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning):** Hyperparameter tuning jobs automate tuning of hyperparameters of your model based on the criteria you configure such as goal/metric to optimize, hyperparameters values and number of trials to run.\n",
    "- **[Training pipelines](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline):** Orchestrates custom training jobs or hyperparameter tuning jobs with additional steps after the training job is successfully completed.\n",
    "\n",
    "Please refer to the [documentation](https://cloud.google.com/vertex-ai/docs/training/custom-training-methods) for further details.\n",
    "\n",
    "In this notebook, we will cover Custom Jobs and Hyperparameter tuning jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361c3fe",
   "metadata": {},
   "source": [
    "### Packaging the training application\n",
    "\n",
    "Before running the training job on Vertex AI, the training application code and any dependencies must be packaged and uploaded to Cloud Storage bucket or Container Registry or Artifact Registry that your Google Cloud project can access. This sections shows how to package and stage your application in the cloud.\n",
    "\n",
    "There are two ways to package your application and dependencies and train on Vertex AI:\n",
    "\n",
    "1. [Create a Python source distribution](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container) with the training code and dependencies to use with a [pre-built containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) on Vertex AI\n",
    "2. Use [custom containers](https://cloud.google.com/ai-platform/training/docs/custom-containers-training) to package dependencies using Docker containers\n",
    "\n",
    "**This notebook shows both packaging options to run a custom training job on Vertex AI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e73a1",
   "metadata": {},
   "source": [
    "#### Recommended Training Application Structure\n",
    "\n",
    "You can structure your training application in any way you like. However, the [following structure](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container#structure) is commonly used in Vertex AI samples, and having your project's organization be similar to the samples can make it easier for you to follow the samples.\n",
    "\n",
    "We have two directories `python_package` and `custom_container` showing both the packaging approaches. `README.md` files inside each directory has details on the directory structure and instructions on how to run application locally and on the cloud.\n",
    "\n",
    "```\n",
    ".\n",
    "├── custom_container\n",
    "│   ├── Dockerfile\n",
    "│   ├── README.md\n",
    "│   ├── scripts\n",
    "│   │   └── train-cloud.sh\n",
    "│   └── trainer -> ../python_package/trainer/\n",
    "├── python_package\n",
    "│   ├── README.md\n",
    "│   ├── scripts\n",
    "│   │   └── train-cloud.sh\n",
    "│   ├── setup.py\n",
    "│   └── trainer\n",
    "│       ├── __init__.py\n",
    "│       ├── experiment.py\n",
    "│       ├── metadata.py\n",
    "│       ├── model.py\n",
    "│       ├── task.py\n",
    "│       └── utils.py\n",
    "└── pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb    --> This notebook\n",
    "```\n",
    "\n",
    "1. Main project directory contains your `setup.py` file or `Dockerfile` with the dependencies. \n",
    "2. Use a subdirectory named `trainer` to store your main application module and `scripts` to submit training jobs locally or cloud\n",
    "3. Inside `trainer` directory:\n",
    "    - `task.py` - Main application module 1) initializes and parse task arguments (hyper parameters), and 2) entry point to the trainer\n",
    "    - `model.py` -  Includes function to create model with a sequence classification head from a pre-trained model.\n",
    "    - `experiment.py` - Runs the model training and evaluation experiment, and exports the final model.\n",
    "    - `metadata.py` - Defines metadata for classification task such as predefined model dataset name, target labels\n",
    "    - `utils.py` - Includes utility functions such as data input functions to read data, save model to GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b28c1",
   "metadata": {},
   "source": [
    "### Run Custom Job on Vertex AI Training with a pre-built container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45fc7d",
   "metadata": {},
   "source": [
    "Vertex AI provides Docker container images that can be run as [pre-built containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#available_container_images) for custom training. These containers include common dependencies used in training code based on the Machine Learning framework and framework version.\n",
    "\n",
    "In this notebook, we are using Hugging Face Datasets and fine tuning a transformer model from Hugging Face Transformers Library for sentiment analysis task using PyTorch. We will use [pre-built container for PyTorch](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#pytorch) and package the training application code by adding standard Python dependencies - `transformers`, `datasets` and `tqdm` - in the `setup.py` file. \n",
    "\n",
    "![Training with Prebuilt Containers on Vertex AI Training](./images/training-with-prebuilt-containers-on-vertex-training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc1505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc85c99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598416a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f67da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c4032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
